# -*- coding: utf-8 -*-
"""ISLR Chapter 1 and 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15cH4AFk2iTDsClQ4tMvRElAPZukSEUv7

# 2.3.1 Basic Commands

## Functions
"""

import numpy as np

x = np.array([1,6,2])

x

y = np.array([1,4,3])

y

"""## Length"""

len(x)

len(y)

x + y

whos

"""## Delete"""

del(x)

# x

"""## Matrix"""

x = np.array([1,2,3,4]).reshape(2, 2)

x.T

np.sqrt(x.T)

x.T**2

np.random.seed(1303)
x = np.random.rand(50)
y = x + np.random.normal(loc=50, scale=0.1, size=50)

from scipy.stats.stats import pearsonr
pearsonr(x,y)

x.mean()

x.var()

np.sqrt(x.var())

x.std()

"""# 2.3.2 Graphics

## Plot
"""

import matplotlib.pyplot as plt

x = np.random.random(100)
y = np.random.random(100)

plt.plot(x,y);

plt.plot(x,y)
plt.xlabel("this is the x-axis")
plt.ylabel("this is the y-axis")
plt.title("Plot of X vs Y");

"""## Jpeg"""

plt.savefig(r'Figure.jpg')

"""## Sequence"""

x = np.arange(1,10)

x

x = np.arange(1,11)

x

x = np.linspace(-1*np.pi, np.pi)

x

y = x

y

"""# 2.3.3 Indexing Data

Difference, the END digit should be PLUS 1 in PYTHON
"""

A = np.arange(1,17).reshape(4,4).T
A

"""Difference, when refferencing should be MINUS 1 for COLUMN and ROW"""

A[1,2]

"""No SPECIFIC command for selecting multiple (non-adjacent) indeces"""

r1 = [1,3]
rx = [row[r1] for row in A]
list(rx)

from operator import itemgetter
r2 = [0,2]
list(itemgetter(*r2)(rx))

A.shape

"""# 2.3.4 Loading Data"""

import pandas as pd
auto = pd.read_csv(r'Auto.csv')

auto.head()

auto.shape

auto.info()

auto.columns

"""# 2.3.5 Additional Graphical and Numerical Summaries"""

plt.scatter(auto.cylinders, auto.mpg, c='r')

auto.mpg.hist(color='red', bins=15)

auto.corr()

import seaborn as sns
sns.pairplot(auto, kind='reg');

auto.describe()

"""# 2.4 Exercises

## 1 For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

### (a) The sample size n is extremely large, and the number of predictors p is small.

Approach: Flexible

### (b) The number of predictors p is extremely large, and the number of observations n is small.

### (c) The relationship between the predictors and response is highly non-linear.

### (d) The variance of the error terms, i.e. Ïƒ2 = Var(), is extremely high.

## 2.  Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p

### (a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

Answer 1: Regression   
Answer 2: Inference  
Answer 3: n = 500, p = 4

### (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables

Answer 1: Classification  
Answer 2: Prediction  
Answer 3: n = 20, p = 14

### (c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

Answer 1: Regression  
Answer 2: Prediction  
Answer 3: n = 48, p = 4

## 3. We now revisit the bias-variance decomposition.

### (a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.

### (b) Explain why each of the five curves has the shape displayed in part (a).

## 4. You will now think of some real-life applications for statistical learning.

### (a) Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

Answer: Cancer (Benign or Malignant). Cancer size (length, width, height), mass of the cell, age of patient, gender of patient, etc. Prediction since you have readily available data.

### (b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer

Answer: Movie Profit. Based on the genre, year, actor, actress, budget, customer rating, etc. This is Prediction since in producing a movie, you are not sure what will be the impact to the profit of the movie.

### (c) Describe three real-life applications in which cluster analysis might be useful.

Answer: Clustering of banks based on their balance sheet

## 5. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

Answer: Very flexible models / approach maybe hard to interpret. Example of these models are Support Vvector Machines and Gradient Boosting Models both for Regression and Classification.  
  
Answer: Real life problems that uses deep learning method (more flexible) or datasets with very large number of observations and variables.  
  
Answer: Usually datasets with less number of predictors and less number of variables prefers less flexible approach.

## 6. Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages?

<b>Benefits of Parametric Machine Learning Algorithms:</b>

Simpler: These methods are easier to understand and interpret results.
Speed: Parametric models are very fast to learn from data.
Less Data: They do not require as much training data and can work well even if the fit to the data is not perfect.
Limitations of Parametric Machine Learning Algorithms:

Constrained: By choosing a functional form these methods are highly constrained to the specified form.
Limited Complexity: The methods are more suited to simpler problems.
Poor Fit: In practice the methods are unlikely to match the underlying mapping function.

<b>Benefits of Nonparametric Machine Learning Algorithms:</b>

Flexibility: Capable of fitting a large number of functional forms.
Power: No assumptions (or weak assumptions) about the underlying function.
Performance: Can result in higher performance models for prediction.
Limitations of Nonparametric Machine Learning Algorithms:

More data: Require a lot more training data to estimate the mapping function.
Slower: A lot slower to train as they often have far more parameters to train.
Overfitting: More of a risk to overfit the training data and it is harder to explain why specific predictions are made.

<i>Taken from: https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/</i>

## 6. The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.

### (a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0.
"""

df = pd.DataFrame([[0,3,0, "red"], [2,0,0,"red"], [0,1,3,'red'], [0,1,2, 'green'], [-1,0,1, 'green'], [1,1,1,'red']],
                  columns=['X1','X2','X3','Y'])

df

import numpy as np

x1 = 0
x2 = 0
x3 = 0
ans = []

for n in np.arange(0, len(df)):
    ans.append((df.X1[n] - x1)**2 + (df.X2[n] - x2)**2 + (df.X3[n] - x3)**2)
    
print(ans)

"""### (b) What is our prediction with K = 1? Why?

Answer: Green. Class assigned to single nearest neighbor. The Eucledian distance when X1=X2=X3=0 = 0. Nearest value is 2 which Y= Green.

### (c) What is our prediction with K = 3? Why?

Answer: Red. Since 2/3 nearest neighbors are Y=Red and only one (1) neighbor is Green. Then we accept the result to be Y=Red.

### (d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?

Answer: Smaller value of K. As K grows, the method becomes less flexible and produces a decision boundary that is close to linear which is moving farther away from the problem which is highly nonlinear.

## Exercise 8 This exercise relates to the College data set, which can be found in the file College.csv.

## a Use the read.csv() function to read the data into R.
## b Look at the data using the fix() function.
"""

import pandas as pd

college = pd.read_csv(r'College.csv', index_col=0)

college.head()

"""## c

### (i) Use the summary() function to produce a numerical summary of the variables in the data set.
"""

college.describe()

"""### (ii) Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data."""

import seaborn as sns

sns.pairplot(college[:11], kind='reg');

"""### (iii) Use the plot() function to produce side-by-side boxplots of Outstate versus Private."""

import matplotlib.pyplot as plt

college['Private'].unique()

box1 = college.loc[college['Private'] == 'Yes']['Outstate']
box2 = college.loc[college['Private'] == 'No']['Outstate']

fig, ax = plt.subplots(1,2, figsize=(20,5))
ax[0].boxplot(box1);
ax[0].set_xlabel('Yes')
ax[1].boxplot(box2);
ax[1].set_xlabel('No');

"""### (iv)  Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10 % of their high school classes exceeds 50 %."""

college['Elite'] = college['Top10perc'].apply(lambda x: 'Yes' if x > 50 else 'No')

college['Elite'].value_counts()

elite_box1 = college.loc[college['Elite'] == 'Yes']['Outstate']
elite_box2 = college.loc[college['Elite'] == 'No']['Outstate']

fig, ax = plt.subplots(1,2, figsize=(20,5))
ax[0].boxplot(elite_box1);
ax[0].set_xlabel('Yes')
ax[1].boxplot(elite_box2);
ax[1].set_xlabel('No');

fig, ax = plt.subplots(1,2, figsize=(12,5))
sns.boxplot(elite_box1, orient='v', ax=ax[0])
ax[0].set_xlabel('Yes')
sns.boxplot(elite_box2, orient='v', ax=ax[1])
ax[1].set_xlabel('No');

sns.boxplot(college['Elite'], college['Outstate'], orient='v');

"""### (v) Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables"""

college.describe().columns

fig, ax = plt.subplots(2,2, figsize=(15,10))
sns.distplot(college['Apps'], ax=ax[0,0]);
sns.distplot(college['Accept'], ax=ax[0,1]);
sns.distplot(college['Enroll'], ax=ax[1,0]);
sns.distplot(college['Top10perc'], ax=ax[1,1]);

"""## Exercise 9  This exercise involves the Auto data set studied in the lab."""

import pandas as pd
Auto = pd.read_csv(r'Auto.csv')

Auto.head()

Auto.info()

Auto[Auto['horsepower'] == '?']

Auto[Auto['name'] == '?']

Auto_new = Auto[Auto['horsepower'] != '?']

len(Auto)

len(Auto_new)

Auto_new['horsepower'] = Auto_new['horsepower'].astype('int32')

Auto_new.columns

Auto_new.describe()

"""### Answers

### (a) Which of the predictors are quantitative, and which are qualitative? 
### (b) What is the range of each quantitative predictor? You can answer this using the range() function
"""

Auto_new.describe().loc[['min', 'max']]

"""### (c) What is the mean and standard deviation of each quantitative predictor?"""

Auto_new.describe().loc[['mean', 'std']]

Auto_new[:9]

"""### (d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?"""

Auto_new_0_9 = Auto_new[:9]

Auto_new_0_9.describe().loc[['min', 'max','mean', 'std']]

"""### (e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings."""

import seaborn as sns
sns.pairplot(Auto_new, kind='reg');

"""### (f) Suppose we with to predict "mpg", from the plot below, the variables "cylinder", "displacement", "horsepower" and "weight" are the variables higly correlated to "mpg"."""

from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

def corrfunc(x, y, **kws):
    r, _ = stats.pearsonr(x, y)
    ax = plt.gca()
    ax.annotate("r = {:.2f}".format(r),
                xy=(.1, .9), xycoords=ax.transAxes)
    
g = sns.PairGrid(Auto_new)
g.map_upper(plt.scatter, s=10)
g.map_diag(sns.distplot, kde=False)
g.map_lower(sns.kdeplot, cmap="Blues_d")
g.map_lower(corrfunc)

"""## Exercise 10 This exercise involves the Boston housing data set."""

from sklearn.datasets import load_boston
boston_data = load_boston()

df_boston = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)
df_boston['target'] = pd.Series(boston_data.target)
df_boston.head()

print(boston_data.DESCR)

"""### (a)"""

import pandas as pd

Boston = pd.read_csv(r'Boston.csv', index_col=0)

Boston.head()

len(Boston.columns)

len(Boston)

"""### (b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings."""

import seaborn as sns
from matplotlib import rcParams
# sns.set(rc={'figure.figsize':(50,50)})
rcParams['figure.figsize'] = 20,20
sns.pairplot(Boston);

"""Given the pairplot above, we can see that there are some illustrations in which two (2) variables are linearly correlated.

### (c)  Are any of the predictors associated with per capita crime rate? If so, explain the relationship
"""

Boston.columns

"""Given the pairplot above and looking at the outer left column (crim), we can say that only a few variables can be considered as good predictors for the "crim" variable. Wih pearson R correlation performed below, only a few variables are moderately correlated and these are: "indus", "nox", "rad", "tax" and "lstat"."""

Boston.corr()['crim']

"""### (d) Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor"""

import matplotlib.pyplot as plt
import seaborn as sns

fig, ax = plt.subplots(1,3, figsize=(20,5))
sns.boxplot(Boston.crim, orient='v', ax=ax[0])
ax[0].set_title('Crime Rate')
sns.boxplot(Boston.tax, orient='v', ax=ax[1])
ax[1].set_title('Tax Rates')
sns.boxplot(Boston.ptratio, orient='v', ax=ax[2])
ax[2].set_title('Pupil-teacher Ratios');

"""### (d) Crime Rate

Looking at the Crime Rate box plot, a few towns in Boston have high particular crime rates. Most of the crime rates are on 5 units and less than. If we look at those towns with high crime rates, and looking at each of the predictors, we can see that :
1. "zn"   - small land proportions
2. "indus" - very large non-retail business
3. "age" - very old establishments
4. "black" - large distribution of black people
"""

Boston[Boston['crim'] > 10].describe().loc[['min', 'max', 'mean']]

"""### (d) Tax Rate"""

Boston.describe().loc[['mean']]

"""Looking at the Tax Rate box plot, most of the towns have higher tax rates from the average."""

Boston[Boston['tax'] > 400].describe().loc[['min', 'max', 'mean']]

"""### (d) Pupil-Teacher Ratio"""

Boston[Boston['ptratio'] > 18].describe().loc[['min', 'max', 'mean']]

"""### (e) How many of the suburbs in this data set bound the Charles river?"""

Boston.chas.unique()

len(Boston[Boston.chas ==1])

"""### (f) What is the median pupil-teacher ratio among the towns in this data set?"""

Boston.ptratio.median()

"""### (g) Which suburb of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your findings."""

Boston[Boston.medv == Boston.medv.min()]

Boston.describe().loc[['min', 'max']]

"""Comparison:
1. Crime rate are on the higher values.
2. Small residential land zone
3. Non-retail close to average
4. Not bounded by the river
5. High nitric oxides concentration
6. On the average number of rooms
7. Old establishments
8. Far from Boston employment centers
9. Far from radial highways
10. On the lower average of tax ratio
11. On the lower average of pupil-teacher ration
12. High number of black people
13. On the lower status of population

### (h) In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling
"""

len(Boston[Boston.rm > 7])

len(Boston[Boston.rm > 8])

Boston[Boston.rm > 8].describe().loc[['min', 'max']]

Boston.describe().loc[['min', 'max']]

"""Comments:
1. Lower crime rates
2. mid-high average tax-ratio
3. High number of black people
4. Lower status of population
"""

